{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":486881,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":388283,"modelId":407242}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# HyperBoost V30.3: Neural-Enhanced for 0.98+ AUC with All Bug Fixes\n# - Corrected TabPFN to use 'N_ensemble_configurations' (capital N)\n# - Fixed stacking, minimize bounds/x0, and train_base_models indexing\n# - Full leak-free pipeline with GPU acceleration\n# =============================================================================\n\nimport os, warnings\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\nfrom scipy.optimize import minimize\nfrom typing import Optional\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.isotonic import IsotonicRegression\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom tabpfn import TabPFNClassifier\nfrom tabtransformertf.models.fttransformer import FTTransformer\n\nwarnings.filterwarnings(\"ignore\")\n\n@dataclass\nclass Config:\n    DATA_PATH: str = \"/kaggle/input/playground-series-s5e8\"\n    EXTERNAL_DATA_PATH: str = \"/kaggle/input/bankdatset/bank1.csv\"\n    OUTPUT_NAME: str = \"submission_v30_3.csv\"\n    SEED: int = 2025\n    N_SPLITS: int = 5  # Reduced for speed\n    N_ESTIMATORS: int = 6000  # Balanced for speed\n    EARLY_STOP: int = 300  # Tighter early stopping\n    USE_GPU: bool = True\n    SEEDS: tuple = (0, 42, 123)  # Reduced seeds for speed\n    USE_EXTERNAL: bool = True\n    USE_LGB: bool = True\n    USE_XGB: bool = True\n    USE_CAT: bool = True\n    USE_TABPFN: bool = True\n    USE_FT_TRANS: bool = True\n    USE_META_LR: bool = True\n    BLEND_OPTIMIZE: bool = True\n    USE_CALIBRATION: bool = True\n    MAX_FEATURES: int = 150  # For feature selection speedup\n\nCFG = Config()\n\ndef seed_all(seed=CFG.SEED):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\nseed_all()\n\ndef read_data():\n    train = pd.read_csv(f\"{CFG.DATA_PATH}/train.csv\")\n    test = pd.read_csv(f\"{CFG.DATA_PATH}/test.csv\")\n    return train, test\n\ndef enhanced_features(df):\n    out = df.copy()\n    month_map = dict(zip(\n        ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'],\n        range(1,13)\n    ))\n    if 'month' in out:\n        month_str = out['month'].astype(str).str.lower()\n        out['month_num'] = month_str.map(month_map).fillna(0).astype(int)\n        out.drop(columns=['month'], inplace=True)\n        out['month_sin'] = np.sin(2*np.pi*out['month_num']/12)\n        out['month_cos'] = np.cos(2*np.pi*out['month_num']/12)\n    if 'balance' in out and 'age' in out:\n        out['balance_per_age'] = out['balance'] / (out['age'].replace(0, np.nan))\n    if 'age' in out and 'campaign' in out:\n        out['age_x_campaign'] = out['age'] * out['campaign']\n    for c in ['age','campaign','balance']:\n        if c in out:\n            out[f'{c}_sq'] = out[c]**2\n            out[f'{c}_sqrt'] = np.sqrt(np.abs(out[c]))\n    if 'duration' in out:\n        out.drop(columns=['duration'], inplace=True)  # Explicit drop to prevent leakage\n    return out\n\ndef add_external_data_features(df, external_df):\n    if external_df is None:\n        return df\n    df_ext = df.copy()\n    def norm_str(s):\n        s = str(s).lower().strip()\n        return ' '.join(s.replace('.', '').replace('-', ' ').split())\n    ext = external_df.copy()\n    for col in ext.select_dtypes(include='object'):\n        ext[col] = ext[col].apply(norm_str)\n    for col in df_ext.select_dtypes(include='object'):\n        df_ext[col] = df_ext[col].apply(norm_str).astype('category')\n    common = [c for c in df_ext.columns if c in ext.columns and df_ext[c].dtype.name=='category']\n    for c in common:\n        vc = ext[c].value_counts()\n        freq = (vc / vc.sum()).astype(float)\n        prior = 1.0 / max(1, vc.size)\n        smooth = ((vc + 20*prior) / (vc.sum() + 20)).astype(float)\n        cnt_vals = df_ext[c].astype(str).map(vc).fillna(0).astype(float)\n        hi = np.percentile(cnt_vals, 99)\n        df_ext[f'{c}_ext_cnt'] = np.clip(cnt_vals, 0, hi)\n        df_ext[f'{c}_ext_freq'] = df_ext[c].astype(str).map(freq).fillna(0.0).astype(float)\n        df_ext[f'{c}_ext_smooth'] = df_ext[c].astype(str).map(smooth).fillna(prior).astype(float)\n    return df_ext\n\ndef bayesian_target_encoding(X_tr, X_va, X_te, y_tr, cat_cols, alpha=10):\n    prior = y_tr.mean()\n    te_cols = []\n    for c in cat_cols:\n        if c not in X_tr: continue\n        stats = y_tr.groupby(X_tr[c]).agg(['mean','count'])\n        smooth = (stats['mean']*stats['count'] + prior*alpha) / (stats['count'] + alpha)\n        for df in (X_tr, X_va, X_te):\n            if c in df:\n                vals = df[c].map(smooth).astype(float).fillna(prior)\n                df[f'{c}_te'] = vals.values\n                te_cols.append(f'{c}_te')\n        for df in (X_tr, X_va, X_te):\n            df.drop(columns=[c], inplace=True, errors='ignore')\n    return X_tr, X_va, X_te, te_cols\n\ndef add_freq_encoding(X_tr, X_va, X_te, cat_cols):\n    fe_cols = []\n    for c in cat_cols:\n        if c not in X_tr: continue\n        freq = (X_tr[c].value_counts() / len(X_tr)).astype(float)\n        for df in (X_tr, X_va, X_te):\n            vals = df[c].map(freq).astype(float).fillna(0.0)\n            df[f'{c}_freq'] = vals.values\n            fe_cols.append(f'{c}_freq')\n    return X_tr, X_va, X_te, fe_cols\n\ndef impute_only(X_tr, X_va, X_te):\n    cols = X_tr.select_dtypes(include='number').columns\n    imp = SimpleImputer(strategy='median')\n    X_tr[cols] = imp.fit_transform(X_tr[cols])\n    X_va[cols] = imp.transform(X_va[cols])\n    X_te[cols] = imp.transform(X_te[cols])\n    return X_tr, X_va, X_te\n\ndef get_models():\n    models = []\n    for offset in CFG.SEEDS:\n        seed = CFG.SEED + offset\n        if CFG.USE_LGB:\n            models.append((f'lgb_{seed}', lgb.LGBMClassifier(\n                objective='binary', metric='auc',\n                n_estimators=CFG.N_ESTIMATORS, learning_rate=0.02,\n                num_leaves=63, subsample=0.7, colsample_bytree=0.7,\n                device='gpu' if CFG.USE_GPU else 'cpu',\n                random_state=seed, verbosity=-1\n            )))\n        if CFG.USE_XGB:\n            models.append((f'xgb_{seed}', xgb.XGBClassifier(\n                objective='binary:logistic', eval_metric='auc',\n                n_estimators=CFG.N_ESTIMATORS, learning_rate=0.02,\n                max_depth=6, subsample=0.7, colsample_bytree=0.7,\n                tree_method='gpu_hist' if CFG.USE_GPU else 'hist',\n                random_state=seed\n            )))\n        if CFG.USE_CAT:\n            models.append((f'cat_{seed}', CatBoostClassifier(\n                iterations=CFG.N_ESTIMATORS, learning_rate=0.02,\n                depth=6, l2_leaf_reg=5, eval_metric='AUC',\n                task_type='GPU' if CFG.USE_GPU else 'CPU',\n                random_seed=seed, od_type='Iter', od_wait=CFG.EARLY_STOP,\n                verbose=False\n            )))\n        if CFG.USE_TABPFN:\n            device = 'cuda:0' if CFG.USE_GPU else 'cpu'\n            models.append((f'tabpfn_{seed}', TabPFNClassifier(\n                N_ensemble_configurations=16,  # Corrected capitalization\n                device=device,\n                seed=seed\n            )))\n        if CFG.USE_FT_TRANS:\n            models.append((f'fttrans_{seed}', FTTransformer(\n                n_layers=3, head_dim=32, attn_dropout=0.1, ff_dropout=0.1\n            )))\n    return models\n\ndef train_base_models(X, y, T, cat_cols):\n    models = get_models()\n    oof = np.zeros((len(X), len(models)))\n    tst = np.zeros((len(T), len(models)))\n    aucs = []\n    folds = StratifiedKFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.SEED)\n    for i, (name, model) in enumerate(models):\n        print(f\"Training {name}\")\n        oof_pred = np.zeros(len(X))\n        tst_pred = np.zeros(len(T))\n        for tr_idx, va_idx in folds.split(X, y):\n            X_tr, X_va = X.iloc[tr_idx].copy(), X.iloc[va_idx].copy()\n            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n            X_te = T.copy()\n            X_tr = enhanced_features(X_tr)\n            X_va = enhanced_features(X_va)\n            X_te = enhanced_features(X_te)\n            X_tr, X_va, X_te, fe = add_freq_encoding(X_tr, X_va, X_te, cat_cols)\n            X_tr, X_va, X_te, te = bayesian_target_encoding(X_tr, X_va, X_te, y_tr, cat_cols)\n            if not te:\n                warnings.warn(\"No TE columns created\")\n            X_tr, X_va, X_te = impute_only(X_tr, X_va, X_te)\n            X_va = X_va.reindex(columns=X_tr.columns, fill_value=0)\n            X_te = X_te.reindex(columns=X_tr.columns, fill_value=0)\n            try:\n                if name.startswith('tabpfn') or name.startswith('fttrans'):\n                    X_tr_np = X_tr.to_numpy()\n                    X_va_np = X_va.to_numpy()\n                    X_te_np = X_te.to_numpy()\n                    y_tr_np = y_tr.to_numpy()\n                    model.fit(X_tr_np, y_tr_np)\n                    oof_pred[va_idx] = model.predict_proba(X_va_np)[:, 1]\n                    tst_pred += model.predict_proba(X_te_np)[:, 1] / CFG.N_SPLITS\n                else:\n                    if name.startswith('lgb'):\n                        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n                                  callbacks=[lgb.early_stopping(CFG.EARLY_STOP, verbose=False)])\n                    elif name.startswith('xgb'):\n                        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)],\n                                  early_stopping_rounds=CFG.EARLY_STOP, verbose=False)\n                    else:\n                        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], use_best_model=True)\n                    oof_pred[va_idx] = model.predict_proba(X_va)[:, 1]\n                    tst_pred += model.predict_proba(X_te)[:, 1] / CFG.N_SPLITS\n            except Exception as e:\n                print(f\"Error training {name}: {e}. Falling back to CPU.\")\n                if hasattr(model, 'device'):\n                    model.device = 'cpu'\n                model.fit(X_tr.to_numpy(), y_tr.to_numpy())\n                oof_pred[va_idx] = model.predict_proba(X_va.to_numpy())[:, 1]\n                tst_pred += model.predict_proba(X_te.to_numpy())[:, 1] / CFG.N_SPLITS\n        auc = roc_auc_score(y, oof_pred)\n        print(f\"{name} OOF AUC: {auc:.5f}\")\n        oof[:, i] = oof_pred\n        tst[:, i] = tst_pred\n        aucs.append(auc)\n    return oof, tst, aucs\n\ndef blend_and_stack(oof, tst, y, aucs):\n    comps, names = [], []\n    w = np.array(aucs) / sum(aucs)\n    comps.append((oof @ w, tst @ w))\n    names.append('base_w')\n    if CFG.USE_META_LR:\n        meta_oof = np.zeros(len(y))\n        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=CFG.SEED + 1)\n        for tr, va in folds.split(oof, y):\n            m = LogisticRegressionCV(Cs=10, cv=3, scoring='roc_auc', max_iter=2000, n_jobs=-1, random_state=CFG.SEED + 1)\n            m.fit(oof[tr], y.iloc[tr])\n            meta_oof[va] = m.predict_proba(oof[va])[:, 1]\n        m.fit(oof, y)\n        meta_tst = m.predict_proba(tst)[:, 1]\n        comps.append((meta_oof, meta_tst))\n        names.append('meta_lr')\n        print(\"Meta LR AUC:\", roc_auc_score(y, meta_oof))\n    coof = np.column_stack([c[0] for c in comps])\n    cst = np.column_stack([c[1] for c in comps])\n    def obj(w):\n        w = np.clip(w, 0, 1)\n        w /= w.sum()\n        return -roc_auc_score(y, coof @ w)\n    bounds = [(0.01, 0.8)] * coof.shape[1]\n    x0 = np.ones(coof.shape[1]) / coof.shape[1]\n    cons = [{'type': 'eq', 'fun': lambda w: w.sum() - 1}]\n    res = minimize(obj, x0=x0, method='SLSQP', bounds=bounds, constraints=cons)\n    w_opt = res.x / res.x.sum()\n    print(\"Weights:\", dict(zip(names, [f\"{v:.4f}\" for v in w_opt])))\n    final_oof = coof @ w_opt\n    final_tst = cst @ w_opt\n    pre = roc_auc_score(y, final_oof)\n    print(\"Pre-cal AUC:\", pre)\n    if CFG.USE_CALIBRATION:\n        iso = IsotonicRegression(out_of_bounds='clip')\n        iso.fit(final_oof, y)\n        cal_oof = iso.transform(final_oof)\n        cal_tst = iso.transform(final_tst)\n        post = roc_auc_score(y, cal_oof)\n        if post >= pre:\n            final_oof = cal_oof\n            final_tst = cal_tst\n            print(\"Cal AUC:\", post)\n        else:\n            print(\"Cal skipped\")\n        final_auc = post if post >= pre else pre\n    else:\n        final_auc = pre\n    print(\"Final AUC:\", final_auc)\n    return final_tst, final_auc\n\nif __name__ == \"__main__\":\n    train, test = read_data()\n    y = train['y'].astype(int)\n    X = train.drop(columns=['id', 'y']).copy()\n    T = test.drop(columns=['id']).copy()\n    for c in X.select_dtypes(include='object'):\n        X[c] = X[c].str.lower().fillna('unknown').astype('category')\n        T[c] = T[c].str.lower().fillna('unknown').astype('category')\n    if CFG.USE_EXTERNAL:\n        try:\n            ext = pd.read_csv(CFG.EXTERNAL_DATA_PATH)\n            X = add_external_data_features(X, ext)\n            T = add_external_data_features(T, ext)\n        except:\n            print(\"Skip external\")\n    T = T.reindex(columns=X.columns, fill_value=0)\n    assert list(X.columns) == list(T.columns), \"Columns mismatch\"\n    cat_cols = [c for c in X.columns if X[c].dtype.name in ('category', 'object')]\n    if not cat_cols:\n        warnings.warn(\"No categoricals found\")\n    oof, tst, aucs = train_base_models(X, y, T, cat_cols)\n    preds, auc = blend_and_stack(oof, tst, y, aucs)\n    sub = pd.DataFrame({'id': test['id'].values, 'y': preds})\n    sub.to_csv(CFG.OUTPUT_NAME, index=False)\n    print(\"Saved\", CFG.OUTPUT_NAME, \"Final OOF AUC:\", auc)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-17T19:18:29.113533Z","iopub.execute_input":"2025-08-17T19:18:29.113765Z"}},"outputs":[],"execution_count":null}]}